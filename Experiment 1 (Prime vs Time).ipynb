{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5616,"status":"ok","timestamp":1691223930621,"user":{"displayName":"Shelby Mo","userId":"15687955477316906745"},"user_tz":420},"id":"EuSUqKeNoGCS","outputId":"adc1155b-6d59-43ee-c8d7-e6baee79d06b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Sat Aug  5 08:25:30 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   37C    P8    10W /  70W |      3MiB / 15360MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["import torch\n","from torch import nn\n","import torch.nn.functional as F\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","from torch.optim.lr_scheduler import LambdaLR\n","\n","# Define the device for computation\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') \u003e= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)\n","\n","\n","class MLP(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, output_dim):\n","        super(MLP, self).__init__()\n","        self.fc1 = nn.Linear(input_dim, hidden_dim)\n","        self.fc2 = nn.Linear(hidden_dim, output_dim)\n","# # Xavier initialization\n","#         torch.nn.init.xavier_uniform_(self.fc1.weight)\n","#         torch.nn.init.xavier_uniform_(self.fc2.weight)\n","# Normal distribution initialization\n","        torch.nn.init.normal_(self.fc1.weight)\n","        torch.nn.init.normal_(self.fc2.weight)\n","    def forward(self, x):\n","        x = self.fc1(x)\n","        x = F.relu(x)\n","        x = self.fc2(x)\n","        return x\n","\n","\n","\n","def to_one_hot(i, n):\n","    \"\"\"Convert an integer to one-hot vector\"\"\"\n","    a = torch.zeros(n)\n","    a[i] = 1\n","    return a\n","\n","def from_one_hot(a):\n","    \"\"\"Convert a one-hot vector to an integer\"\"\"\n","    return torch.argmax(a).item()\n","\n","def generate_dataset_one_hot(p, alpha, seed):\n","    \"\"\"Generate a dataset with one-hot encoded inputs and outputs\"\"\"\n","    eye = torch.eye(p)\n","    dataset = []\n","    for a in range(p):\n","        for b in range(p):\n","            c = (a + b) % p\n","            ab_encoded = torch.cat((eye[a], eye[b]))\n","            c_encoded = to_one_hot(c, p)\n","            dataset.append((ab_encoded, c_encoded))\n","    dataset = [(ab.clone().detach(), c.clone().detach()) for ab, c in dataset]\n","    np.random.seed(seed) #Ensures reproducibility, can be changed by changing seed=42\n","    np.random.shuffle(dataset)\n","    train_set, test_set = train_test_split(dataset, train_size=alpha, shuffle=False)\n","    train_input, train_output = zip(*train_set)\n","    test_input, test_output = zip(*test_set)\n","    return list(train_input), list(train_output), list(test_input), list(test_output)\n","\n","def decode_dataset(train_input, train_output, test_input, test_output):\n","    \"\"\"Decode the dataset from one-hot encoded vectors to integers\"\"\"\n","    train_input_decoded = [(from_one_hot(a[:p]), from_one_hot(a[p:])) for a in train_input]\n","    train_output_decoded = [from_one_hot(c) for c in train_output]\n","    test_input_decoded = [(from_one_hot(a[:p]), from_one_hot(a[p:])) for a in test_input]\n","    test_output_decoded = [from_one_hot(c) for c in test_output]\n","    return train_input_decoded, train_output_decoded, test_input_decoded, test_output_decoded\n","\n","\n","def train_network(model, train_input, train_output, test_input, test_output, n_epochs, learning_rate):\n","   # Convert lists to tensors and move to device\n","    train_input = torch.stack(train_input).to(device)\n","    train_output = torch.stack(train_output).to(device)\n","    test_input = torch.stack(test_input).to(device)\n","    test_output = torch.stack(test_output).to(device)\n","\n","    criterion = nn.CrossEntropyLoss()\n","\n","    # Define the optimizer as AdamW\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, betas=(0.9, 0.98), weight_decay=wd)\n","\n","\n","    scheduler = LambdaLR(optimizer, lambda step: min(step/100, 1))\n","\n","    train_losses = []\n","    test_losses = []\n","    train_accuracies = []\n","    test_accuracies = []\n","\n","    for epoch in range(n_epochs):\n","        model.train()\n","        optimizer.zero_grad()\n","        outputs_train = model(train_input)\n","        loss_train = criterion(outputs_train, train_output)\n","        loss_train.backward()\n","        optimizer.step()\n","        scheduler.step()\n","\n","        model.eval()\n","        with torch.no_grad():\n","            outputs_test = model(test_input)\n","            loss_test = criterion(outputs_test, test_output)\n","\n","        train_losses.append(loss_train.item())\n","        test_losses.append(loss_test.item())\n","\n","        predicted_train = torch.argmax(outputs_train.data, dim=1)\n","        predicted_test = torch.argmax(outputs_test.data, dim=1)\n","        train_output_indices = torch.argmax(train_output, dim=1)\n","        test_output_indices = torch.argmax(test_output, dim=1)\n","        train_correct = (predicted_train == train_output_indices).sum().item()\n","        test_correct = (predicted_test == test_output_indices).sum().item()\n","        train_acc = train_correct / len(train_output)\n","        test_acc = test_correct / len(test_output)\n","        train_accuracies.append(train_acc)\n","        test_accuracies.append(test_acc)\n","        if (epoch + 1) % 100 == 0:\n","            print(f'Epoch [{epoch+1}/{n_epochs}], Train Loss: {loss_train.item():.8f}, Test Loss: {loss_test.item():.8f}, Train Acc: {train_acc*100:.2f}%, Test Acc: {test_acc*100:.2f}%')\n","\n","    return train_losses, test_losses, train_accuracies, test_accuracies"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1691223930622,"user":{"displayName":"Shelby Mo","userId":"15687955477316906745"},"user_tz":420},"id":"PCCV72sjuLRp"},"outputs":[],"source":["def create_model(p, hidden_dim=256):\n","    input_dim = 2 * p\n","    output_dim = p\n","    model = MLP(input_dim, hidden_dim, output_dim).to(device)  # Move the model to the device\n","    return model\n","\n","def run_experiment(model, p, alpha, n_epochs, learning_rate, seed, train_network):\n","    # Generate the dataset with one-hot encoded outputs\n","    train_input, train_output, test_input, test_output = generate_dataset_one_hot(p, alpha, seed)\n","\n","    train_losses, test_losses, train_accuracies, test_accuracies = train_network(\n","        model, train_input, train_output, test_input, test_output, n_epochs, learning_rate\n","    )\n","\n","    return train_losses, test_losses, train_accuracies, test_accuracies\n","\n","def plot_loss(train_losses, test_losses, p, seed, run):\n","    plt.figure(figsize=(4, 4))\n","    plt.plot(range(1, len(train_losses)+1), train_losses, label='Train Loss')\n","    plt.plot(range(1, len(test_losses)+1), test_losses, label='Test Loss')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Loss')\n","    plt.grid(True)\n","    plt.yscale('log')  # Make y-axis logarithmic\n","    plt.legend()\n","    plt.title(f'Loss for p={p}, seed={seed}, run={run}')\n","    #plt.savefig(f\"{images_dir}/loss_p_{p}_seed_{seed}_run_{run}.png\")\n","    plt.close()\n","\n","def plot_accuracy(train_accuracies, test_accuracies, p, seed, run):\n","    plt.figure(figsize=(12, 6))\n","    plt.plot(range(1, len(train_accuracies)+1), train_accuracies, label='Train Accuracy')\n","    plt.plot(range(1, len(test_accuracies)+1), test_accuracies, label='Test Accuracy')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Accuracy')\n","    plt.grid(True)\n","    plt.legend()\n","    plt.title(f'Accuracy for p={p}, seed={seed}, run={run}')\n","   # plt.savefig(f'{images_dir}/accuracy_p_{p}_seed_{seed}_run_{run}.png')\n","    plt.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"vcX4Oz4Mugti"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","Running experiment for p=347, seed=42, run=1...\n","Epoch [100/700], Train Loss: 5.55632401, Test Loss: 6.14081049, Train Acc: 2.64%, Test Acc: 0.07%\n","Epoch [200/700], Train Loss: 5.06649494, Test Loss: 6.10437679, Train Acc: 9.95%, Test Acc: 0.07%\n","Epoch [300/700], Train Loss: 0.16031793, Test Loss: 0.47058153, Train Acc: 100.00%, Test Acc: 99.00%\n","Epoch [400/700], Train Loss: 0.04935727, Test Loss: 0.12693599, Train Acc: 100.00%, Test Acc: 99.99%\n","Epoch [500/700], Train Loss: 0.01980856, Test Loss: 0.05191702, Train Acc: 100.00%, Test Acc: 100.00%\n","Epoch [600/700], Train Loss: 0.00864764, Test Loss: 0.02370662, Train Acc: 100.00%, Test Acc: 100.00%\n","Epoch [700/700], Train Loss: 0.00388344, Test Loss: 0.01128101, Train Acc: 100.00%, Test Acc: 100.00%\n","Running experiment for p=347, seed=42, run=2...\n","Epoch [100/700], Train Loss: 5.56411314, Test Loss: 6.14011288, Train Acc: 2.50%, Test Acc: 0.05%\n","Epoch [200/700], Train Loss: 5.21131134, Test Loss: 6.14560795, Train Acc: 7.59%, Test Acc: 0.02%\n","Epoch [300/700], Train Loss: 0.17364661, Test Loss: 0.49083573, Train Acc: 100.00%, Test Acc: 98.94%\n","Epoch [400/700], Train Loss: 0.05246329, Test Loss: 0.13511829, Train Acc: 100.00%, Test Acc: 99.99%\n","Epoch [500/700], Train Loss: 0.02086304, Test Loss: 0.05455160, Train Acc: 100.00%, Test Acc: 100.00%\n","Epoch [600/700], Train Loss: 0.00902313, Test Loss: 0.02492133, Train Acc: 100.00%, Test Acc: 100.00%\n","Epoch [700/700], Train Loss: 0.00404826, Test Loss: 0.01210638, Train Acc: 100.00%, Test Acc: 100.00%\n","Running experiment for p=347, seed=56, run=1...\n","Epoch [100/700], Train Loss: 5.55715561, Test Loss: 6.14326954, Train Acc: 2.64%, Test Acc: 0.08%\n","Epoch [200/700], Train Loss: 5.09236813, Test Loss: 6.14998102, Train Acc: 9.35%, Test Acc: 0.04%\n","Epoch [300/700], Train Loss: 0.15225676, Test Loss: 0.41005939, Train Acc: 100.00%, Test Acc: 99.51%\n","Epoch [400/700], Train Loss: 0.04834103, Test Loss: 0.11891454, Train Acc: 100.00%, Test Acc: 99.99%\n","Epoch [500/700], Train Loss: 0.01926052, Test Loss: 0.04777889, Train Acc: 100.00%, Test Acc: 100.00%\n"]}],"source":["# mount drive\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","images_dir = '/content/gdrive/My Drive/Images'\n","\n","\n","# Define parameters\n","#prime_numbers = [61, 89,97,113,131,157,193,239,269,277,307,331]\n","prime_numbers = [347, 367, 389,419,421] #More Primes\n","#prime_numbers = [967] #even More Primes\n","\n","seeds = [42,56,12]\n","alpha = 0.6  # The ratio of the dataset to use for training\n","n_epochs = 700  # Set to a small number for testing. Increase to desired number for full training.\n","learning_rate = 0.022  # The learning rate for the optimizer\n","wd= 1\n","\n","# Initialize dictionary to store results\n","results = {}\n","\n","# Loop over prime numbers and seeds\n","for p in prime_numbers:\n","    for seed in seeds:\n","\n","        for run in range(1, 3):\n","          # Create a new model for each prime number and seed\n","            model = create_model(p)\n","            print(f\"Running experiment for p={p}, seed={seed}, run={run}...\")\n","            # Run the experiment\n","            train_losses, test_losses, train_accuracies, test_accuracies = run_experiment(\n","                model, p, alpha, n_epochs, learning_rate, seed, train_network\n","            )\n","            # Store the results\n","            results[(p, seed, run)] = {\n","                \"train_loss\": train_losses,\n","                \"test_loss\": test_losses,\n","                \"train_accuracy\": train_accuracies,\n","                \"test_accuracy\": test_accuracies,\n","            }\n","            # Plot and save the loss and accuracy\n","            plot_loss(train_losses, test_losses, p, seed, run)\n","            plot_accuracy(train_accuracies, test_accuracies, p, seed, run)\n","\n","# Print the keys of the results dictionary to check that the results have been stored correctly\n","print(results.keys())\n","torch.save(results, f'/content/gdrive/My Drive//Files/Research/Experiment/Prime_vs_time/Data/(Width 256)(More Primes) results_lr={learning_rate}_alpha={alpha}_wd={wd}.pt')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aWM_pRU70rfB"},"outputs":[],"source":["torch.save(results, f'/content/gdrive/My Drive//Files/Research/Experiment/Prime_vs_time/Data/results_lr={learning_rate}_alpha={alpha}_wd={wd}.pt')\n"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMdKmWEZP/VomPulu0r7k/d","machine_shape":"hm","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}