# -*- coding: utf-8 -*-
"""(Commented) perfect One-Hot with test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1p6Unr0IpY_DvJ2VokIcczNGfid4jhAbY
"""

import torch
from torch import nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from torch.optim.lr_scheduler import LambdaLR

# Define the device for computation
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

gpu_info = !nvidia-smi
gpu_info = '\n'.join(gpu_info)
if gpu_info.find('failed') >= 0:
  print('Not connected to a GPU')
else:
  print(gpu_info)

class MLP(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(MLP, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, output_dim)
# # Xavier initialization
#         torch.nn.init.xavier_uniform_(self.fc1.weight)
#         torch.nn.init.xavier_uniform_(self.fc2.weight)
# Normal distribution initialization
        torch.nn.init.normal_(self.fc1.weight)
        torch.nn.init.normal_(self.fc2.weight)
    def forward(self, x):
        x = self.fc1(x)
        x = F.relu(x)
        x = self.fc2(x)
        return x

def to_one_hot(i, n):
    """Convert an integer to one-hot vector"""
    a = torch.zeros(n)
    a[i] = 1
    return a

def from_one_hot(a):
    """Convert a one-hot vector to an integer"""
    return torch.argmax(a).item()

def generate_dataset_one_hot(p, alpha, seed=41):
    """Generate a dataset with one-hot encoded inputs and outputs"""
    eye = torch.eye(p)
    dataset = []
    for a in range(p):
        for b in range(p):
            c = (a + b) % p
            ab_encoded = torch.cat((eye[a], eye[b]))
            c_encoded = to_one_hot(c, p)
            dataset.append((ab_encoded, c_encoded))
    dataset = [(ab.clone().detach(), c.clone().detach()) for ab, c in dataset]
    np.random.seed(seed) #Ensures reproducibility, can be changed by changing seed=42
    np.random.shuffle(dataset)
    train_set, test_set = train_test_split(dataset, train_size=alpha, shuffle=False)
    train_input, train_output = zip(*train_set)
    test_input, test_output = zip(*test_set)
    return list(train_input), list(train_output), list(test_input), list(test_output)

def decode_dataset(train_input, train_output, test_input, test_output):
    """Decode the dataset from one-hot encoded vectors to integers"""
    train_input_decoded = [(from_one_hot(a[:p]), from_one_hot(a[p:])) for a in train_input]
    train_output_decoded = [from_one_hot(c) for c in train_output]
    test_input_decoded = [(from_one_hot(a[:p]), from_one_hot(a[p:])) for a in test_input]
    test_output_decoded = [from_one_hot(c) for c in test_output]
    return train_input_decoded, train_output_decoded, test_input_decoded, test_output_decoded

# p = 97  # @param
# alpha = 0.6 #@param

# # Generate the dataset with one-hot encoded outputs
# train_input, train_output, test_input, test_output = generate_dataset_one_hot(p, alpha)

# # Decode the dataset
# train_input_decoded, train_output_decoded, test_input_decoded, test_output_decoded = decode_dataset(
#     train_input, train_output, test_input, test_output
# )

# # Return the first few examples from the decoded dataset for verification
# # print(train_input[:2])
# # print(train_input_decoded[:5], train_output_decoded[:5])

def train_network(model, train_input, train_output, test_input, test_output, n_epochs, learning_rate):
   # Convert lists to tensors and move to device
    train_input = torch.stack(train_input).to(device)
    train_output = torch.stack(train_output).to(device)
    test_input = torch.stack(test_input).to(device)
    test_output = torch.stack(test_output).to(device)

    criterion = nn.CrossEntropyLoss() #@param ["nn.CrossEntropyLoss()", "nn.MSELoss()"] {type:"raw"}
    #criterion = nn.MSELoss()    (Now Working)


    # Define the optimizer as AdamW
    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, betas=(0.9, 0.98), weight_decay=wd)
    #optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0 ,weight_decay=0)
    #optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)
    #in this case, optimization with SGD (w and wo momentum) doesn't work unless I set the lt to be very high, unclear why.

    #Scheduler with warmup steps
    # def lr_lambda(epoch):
    #     if epoch < 2000:
    #         return 1.0
    #     else:
    #         return 10.0
    # scheduler = LambdaLR(optimizer, lr_lambda)

    #step Function Scheduler
    scheduler = LambdaLR(optimizer, lambda step: min(step/100, 1))

    train_losses = []
    test_losses = []
    train_accuracies = []
    test_accuracies = []

    for epoch in range(n_epochs):
        model.train()
        optimizer.zero_grad()
        outputs_train = model(train_input)
        loss_train = criterion(outputs_train, train_output)
        loss_train.backward()
        optimizer.step()
        scheduler.step()

        model.eval()
        with torch.no_grad():
            outputs_test = model(test_input)
            loss_test = criterion(outputs_test, test_output)

        train_losses.append(loss_train.item())
        test_losses.append(loss_test.item())

        predicted_train = torch.argmax(outputs_train.data, dim=1)
        predicted_test = torch.argmax(outputs_test.data, dim=1)
        train_output_indices = torch.argmax(train_output, dim=1)
        test_output_indices = torch.argmax(test_output, dim=1)
        train_correct = (predicted_train == train_output_indices).sum().item()
        test_correct = (predicted_test == test_output_indices).sum().item()
        train_acc = train_correct / len(train_output)
        test_acc = test_correct / len(test_output)
        train_accuracies.append(train_acc)
        test_accuracies.append(test_acc)
        if (epoch + 1) % 100 == 0:
            print(f'Epoch [{epoch+1}/{n_epochs}], Train Loss: {loss_train.item():.8f}, Test Loss: {loss_test.item():.8f}, Train Acc: {train_acc*100:.2f}%, Test Acc: {test_acc*100:.2f}%')

    return train_losses, test_losses, train_accuracies, test_accuracies

p = 97  #@param
alpha = 0.6 #@param
train_input, train_output, test_input, test_output = generate_dataset_one_hot(p, alpha)

input_dim = 2 * p
hidden_dim = 128 #@param
output_dim = p
model = MLP(input_dim, hidden_dim, output_dim).to(device)  # Move the model to the device
print(len(train_output))

n_epochs = 20000 #@param
learning_rate = 0.001 #@param
wd= 0.01 #@param    (For very small wd=0.01 , Lr=0.01, we notice slingshots)
train_losses, test_losses, train_accuracies, test_accuracies = train_network(model, train_input, train_output, test_input, test_output, n_epochs, learning_rate)

plt.figure(figsize=(4, 4))
plt.plot(range(1, len(train_losses)+1), train_losses, label='Train Loss')
plt.plot(range(1, len(test_losses)+1), test_losses, label='Test Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.grid(True)
plt.yscale('log')  # Make y-axis logarithmic
plt.legend()
plt.show()

plt.figure(figsize=(12, 6))
plt.plot(range(1, len(train_accuracies)+1), train_accuracies, label='Train Accuracy')
plt.plot(range(1, len(test_accuracies)+1), test_accuracies, label='Test Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.grid(True)
plt.legend()
plt.show()

#example
# model = MLP(input_dim, hidden_dim, output_dim).to(device)
test_input_element = test_input[221].to(device)
sample_try= model(test_input_element)
element_decoded= [(from_one_hot(test_input_element[:p]), from_one_hot(test_input_element[p:]))]
print(element_decoded)
predicted_try = torch.argmax(sample_try)
print(predicted_try)
